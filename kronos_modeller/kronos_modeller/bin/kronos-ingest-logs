#!/usr/bin/env python3
# (C) Copyright 1996-2018 ECMWF.
# 
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0. 
# In applying this licence, ECMWF does not waive the privileges and immunities 
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""

Kronos data ingestion tool.

Given a path, ingests the available data to produce a kronos cache file (python pickle file).
This cache file may be used in other elements of the kronos process.

"""

import sys
import argparse
import pickle

from kronos_modeller import logreader
from kronos_modeller.logreader.base import LogReader


if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument("path", type=str,
                        help="The path of the data to ingest (either directory or filename). "
                             "Operates recursively on directories.")

    parser.add_argument("--type", "-t", type=str, choices=logreader.ingest_types,
                        help="The type of files to be ingested (see log readers under"
                             "kronos/kronos_modeller/logreader)")

    parser.add_argument("--pattern", "-p", type=str,
                        help="Glob to match files to ingest. Defaults vary by type.")

    parser.add_argument("--output", "-o", default="parse_cache", type=str,
                        help="The output file to store the cached, ingested dataset.")

    parser.add_argument("--workers", "-w", default=10, type=int,
                        help="The number of worker processes to use")

    parser.add_argument("--parser",
                        help="The darshan-parser executable to use if 'type' is 'darshan[3]'")

    parser.add_argument("--labeller", choices=LogReader.available_label_methods,
                        help="Select how ingested jobs are labelled by the LogReaders"
                        "\n - 'None': no label is assigned to the job."
                        "\n - 'directory': job label is log 'dirname'."
                        "\n - 'directory-no-par-serial': job label is log 'dirname', "
                        "(but if dirname is 'parallel' or 'serial', uses dirname "
                        "one level up)."
                        "\n' - directory-file-root': label is file-abs-path without extension."
                        )

    parser.add_argument("--clock_rate", "-c", type=float,
                        help="Clock-rate of the hardware where the data have been profiled " +
                        "(only used if type=allinea)")

    # print the help if no arguments are passed
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    # parse the arguments..
    args = parser.parse_args()

    print("ARGS", args)

    if not args.type:
        print("Ingestion type is required.")
        print("Please supply --type=<type>")
        print("Choices: {}".format(logreader.ingest_types))
        sys.exit(1)

    print("Ingesting file(s) associated with path: {}".format(args.path))

    ingest_config = {

        "cache": False,  # Ensure that we always actively parse if using the parsing-specific tool
        "reparse": True,
        "pool_readers": args.workers,
    }

    # add clock_rate information only when relevant (allinea logs)
    if args.type == "allinea":
        ingest_config["clock_rate"] = args.clock_rate

    if args.parser:
        ingest_config['parser'] = args.parser
    if args.labeller:
        ingest_config['label_method'] = args.labeller
    if args.clock_rate:
        ingest_config['clock_rate'] = args.clock_rate
    if args.pattern:
        ingest_config['pattern'] = args.pattern

    print("Ingest config: {}".format(ingest_config))

    dataset = logreader.ingest_data(args.type, args.path, ingest_config=ingest_config)

    print("Ingestion complete. Writing dump file ...")
    with open(args.output, "wb") as f:
        pickle.dump(dataset, f)
    print("done")
