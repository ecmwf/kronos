#!/usr/bin/env python
# (C) Copyright 1996-2017 ECMWF.
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# In applying this licence, ECMWF does not waive the privileges and immunities
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""
script to generate a summary from the KRF files in the run folder
"""

import os
import json
import argparse
import sys
import strict_rfc3339

from kronos.executor.tools import std_of_list, mean_of_list


list_classes = [
    "main/fc/inigroup",
    "main/fc/ensemble/pf",
    "main/fc/ensemble/cf",
    "main/fc/ensemble/logfiles",
    "main/fc/lag",
]

if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("path_run", type=str, help="Path of the kronos run folder (contains the job-<ID> sub-folders)")
    args = parser.parse_args()

    # check that the run path exists
    if not os.path.exists(args.path_run):
        print "Specified run path does not exist: {}".format(args.path_run)
        sys.exit(-1)

    # check that the run path contains the job sub-folders
    job_dirs = [x for x in os.listdir(args.path_run) if os.path.isdir(os.path.join(args.path_run, x)) and "job-" in x]
    if not job_dirs:
        print "Specified path does not contain any job folder (<job-ID>..)!"
        sys.exit(-1)

    # initialization
    fname_list = []
    dict_name_label = {}
    tot_successful_jobs = 0

    jobs_data_dict = {}
    for job_dir in job_dirs:

        sub_dir_path_abs = os.path.join(args.path_run, job_dir)
        sub_dir_files = os.listdir(sub_dir_path_abs)
        krf_file = [f for f in sub_dir_files if f.endswith('.krf')]

        if krf_file:

            tot_successful_jobs += 1

            map_file_name = krf_file[0]
            map_file_full = os.path.join(sub_dir_path_abs, map_file_name)
            map_base_abs = os.path.splitext(map_file_full)[0]

            input_file_path_abs = os.path.join(sub_dir_path_abs, 'input.json')
            stats_file_path_abs = os.path.join(sub_dir_path_abs, 'statistics.krf')

            # read input file
            with open(input_file_path_abs, 'r') as f:
                json_data_input = json.load(f)

            # read stats file
            with open(stats_file_path_abs, 'r') as f:
                json_data_stats = json.load(f)

            # transfer data from input file to stats file
            json_data_stats['metadata'] = json_data_input['metadata']

            if json_data_input['metadata'].get('workload_name'):

                class_name_root = [name for name in list_classes if name in json_data_input['metadata']['workload_name']][0]

                if "serial" in json_data_input['metadata']['workload_name']:
                    class_name = class_name_root+"/serial"

                elif "parallel" in json_data_input['metadata']['workload_name']:
                    class_name = class_name_root+"/parallel"
                else:
                    class_name = "unknown tag"

                jobs_data_dict.setdefault(class_name, []).append(json_data_stats)

    # ///////////////////// print summary ////////////////////////
    print "{:<52s}".format("\n\nPERFORMANCE SUMMARY:\n")

    print "N successful jobs: {:20d}".format(tot_successful_jobs)

    # overall runtime
    # TODO: to be corrected..
    run_times = [strict_rfc3339.rfc3339_to_timestamp(j["created"]) for jobs in jobs_data_dict.values() for j in jobs]
    overall_runtime = max(run_times) - min(run_times)
    print "Total Runtime [s]: {:20.2f}".format(overall_runtime)

    # Show rates per class
    ordered_classe_names = jobs_data_dict.keys()
    ordered_classe_names.sort()
    for wl_name in ordered_classe_names:

        jobs_in_wl = jobs_data_dict[wl_name]

        # -------- calculate relevant summary measures as needed.. ---------
        data_keys = {
            "cpu": {"label": "FLOPS:     [G/s]", "conv": 1.0/1024**3, "label_sum": "FLOPS:     [G]"},
            "read": {"label": "I/O read:  [GiB/s]", "conv": 1.0/1024**3, "label_sum": "I/O read:  [GiB]"},
            "write": {"label": "I/O write: [GiB/s]", "conv": 1.0/1024**3, "label_sum": "I/O write: [GiB]"},
            "mpi-pairwise": {"label": "MPI p2p:   [GiB/s]", "conv": 1.0/1024**3, "label_sum": "MPI p2p:   [GiB]"},
            "mpi-collective": {"label": "MPI col:   [GiB/s]", "conv": 1.0/1024**3, "label_sum": "MPI col:   [GiB]"}
        }

        perf_data = {}
        for name in data_keys.keys():
            perf_data[name] = [(rank["stats"][name]["count"],
                                rank["stats"][name]["bytes"] if name != "cpu" else 0.0,
                                rank["stats"][name]["elapsed"])
                               for sa in jobs_in_wl for rank in sa["ranks"] if rank["stats"].get(name)]
        # ------------------------------------------------------------------------------------------------

        summary_data = {}
        for k in data_keys.keys():
            summary_data[k] = {"vec": zip(*perf_data[k])}

        # ------------- sorted keys --------------
        sorted_keys = [
            "cpu",
            "read",
            "write",
            "mpi-pairwise",
            "mpi-collective"
        ]
        # -----------------------------------------

        for k in sorted_keys:

            if not summary_data[k]["vec"]:

                summary_data[k]["mean"] = "N/A"
                summary_data[k]["max"] = "N/A"
                summary_data[k]["min"] = "N/A"
                summary_data[k]["std"] = "N/A"
                summary_data[k]["sum"] = "N/A"

            else:
                # counter per sec
                if k == "cpu":

                    sc = data_keys[k]["conv"]
                    vals = [f0/float(f2)*sc for f0, f2 in zip(summary_data[k]["vec"][0], summary_data[k]["vec"][2])]
                    summary_data[k]["mean"] = mean_of_list(vals)
                    summary_data[k]["max"] = max(vals)
                    summary_data[k]["min"] = min(vals)
                    summary_data[k]["std"] = std_of_list(vals)
                    summary_data[k]["sum"] = sum(vals)

                # bytes per sec..
                else:

                    sc = data_keys[k]["conv"]
                    vals = [f1 / float(f2) * sc for f1, f2 in zip(summary_data[k]["vec"][1], summary_data[k]["vec"][2])]
                    mean_val = sum(vals) / float(len(vals))
                    summary_data[k]["mean"] = mean_of_list(vals)
                    summary_data[k]["max"] = max(vals)
                    summary_data[k]["min"] = min(vals)
                    summary_data[k]["std"] = std_of_list(vals)
                    summary_data[k]["sum"] = sum(vals)

        # /////////////////////////////// PRINT SUMMARY DATA ///////////////////////////////////
        _fl = 20

        # write names
        print "\n\n{}".format("-"*(_fl+1)*5)
        print "{:^{l}s}|".format(wl_name, l=(_fl + 1) * 5-1)
        print "{}".format("-"*(_fl+1)*5)

        # write TOTALS per class
        print "{}".format("Totals")
        print "{}".format("-" * (_fl + 1) * 2)
        print "{:<{l}s}|{:^{l}s}|".format("Name", "total", l=_fl)
        print "{}".format("-" * (_fl + 1) * 2)
        for k in sorted_keys:
            if isinstance(summary_data[k]["mean"], str):
                print "{:<{l}s}|{:>{l}}|".format(data_keys[k]["label_sum"], summary_data[k]["sum"], l=_fl)
            else:
                print "{:<{l}s}|{:>{l}f}|".format(data_keys[k]["label_sum"], summary_data[k]["sum"], l=_fl)

        # write RATES per class
        print "{}".format("-" * (_fl + 1) * 2)
        print "{}".format("Rates")
        print "{}".format("-" * (_fl + 1) * 5)
        print "{:<{l}s}|{:^{l}s}|{:^{l}s}|{:^{l}s}|{:^{l}s}|".format("Name",
                                                                     "avg",
                                                                     "max",
                                                                     "min",
                                                                     "std",
                                                                     "latency (est)",
                                                                     "bandwidth (est)",
                                                                     l=_fl)
        print "{}".format("-" * (_fl + 1) * 5)

        for k in sorted_keys:
            if isinstance(summary_data[k]["mean"], str):
                print "{:<{l}s}|{:>{l}}|{:>{l}}|{:>{l}}|{:>{l}}|".format(data_keys[k]["label"],
                                                                         summary_data[k]["mean"],
                                                                         summary_data[k]["max"],
                                                                         summary_data[k]["min"],
                                                                         summary_data[k]["std"],
                                                                         l=_fl)
            else:
                print "{:<{l}s}|{:>{l}f}|{:>{l}f}|{:>{l}f}|{:>{l}f}|".format(data_keys[k]["label"],
                                                                             summary_data[k]["mean"],
                                                                             summary_data[k]["max"],
                                                                             summary_data[k]["min"],
                                                                             summary_data[k]["std"],
                                                                             l=_fl)
        print "{}".format("-" * (_fl + 1) * 5)

