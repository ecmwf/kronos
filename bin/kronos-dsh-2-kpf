#!/usr/bin/env python
# (C) Copyright 1996-2017 ECMWF.
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# In applying this licence, ECMWF does not waive the privileges and immunities
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""
Utility to go through the kronos run folder that has been profiled with Allinea,
and generates a KPF file from the Allinea jsons (requires the map2json tool from Allinea to convert MAP to JSON format)
"""

import os
import subprocess
import json
import argparse
import sys

from kronos.core import logreader
from kronos.core.logreader.darshan import DarshanLogReader3

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from kronos.core.exceptions_iows import ConfigurationError
from kronos.io.profile_format import ProfileFormat
from kronos.core.logreader import profiler_reader

if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("path_run", type=str, help="Path of the kronos run folder (contains the job-<ID> sub-folders)")
    parser.add_argument("kpf_file", type=str, help="Name of the KPF file to write out")
    parser.add_argument("--darshan_parser", '-d', type=str, help="Path of Darshan parser file")
    args = parser.parse_args()

    if not os.path.exists(args.path_run):
        raise ConfigurationError("Specified run path does not exist: {}".format(args.path_run))

    if not args.darshan_parser:
        raise ConfigurationError("Darshan parser not specified!")

    # path of the output kpf file
    path_kpf = os.path.dirname(os.path.abspath(args.kpf_file))
    if not os.path.exists(path_kpf):
        os.mkdir(path_kpf)

    # create a temporary folder where to store all the converted Darshan files..
    tmp_dir_abs = os.path.join(args.path_run, 'tmp')
    if not os.path.exists(tmp_dir_abs):
        os.mkdir(tmp_dir_abs)

    # ---------- Process the run jsons ----------
    print "writing output kpf file: {}".format(args.kpf_file)
    job_dirs = [x for x in os.listdir(args.path_run) if os.path.isdir(os.path.join(args.path_run, x)) and "job-" in x]

    fname_list = []
    dict_name_label = {}

    model_jobs = []
    for job_dir in job_dirs:

        sub_dir_path_abs = os.path.join(args.path_run, job_dir)
        sub_dir_files = os.listdir(sub_dir_path_abs)
        darshan_file = [f for f in sub_dir_files if f.endswith('.darshan')]

        if darshan_file:

            darshan_file_name = darshan_file[0]
            darshan_file_full = os.path.join(sub_dir_path_abs, darshan_file_name)
            darshan_base_abs = os.path.splitext(darshan_file_full)[0]
            print "processing DARSHAN file: {}".format(darshan_file_full)

            parsed_dsh_file_name = darshan_base_abs + '.txt'
            input_file_path_abs = os.path.join(sub_dir_path_abs, 'input.json')

            # read the corresponding json of the input and read the label
            with open(input_file_path_abs, 'r') as f:
                json_data = json.load(f)

            label = json_data['metadata']['workload_name']
            job_ID = json_data['job_num']

            # copy both the darshan's and the converted txt's into the tmp folder..
            darshan_txt = 'darshan_job-' + str(job_ID) + '.txt'
            darshan_darshan = 'darshan_job-' + str(job_ID) + '.darshan'

            with open(parsed_dsh_file_name, "w") as dsh_f:
                subprocess.Popen([args.darshan_parser,
                                 os.path.join(sub_dir_path_abs, darshan_file_name)],
                                 stdout=dsh_f).wait()

            # Backup files
            subprocess.Popen(['cp',
                              os.path.join(sub_dir_path_abs, parsed_dsh_file_name),
                              os.path.join(tmp_dir_abs, darshan_txt)]).wait()

            subprocess.Popen(['cp',
                              os.path.join(sub_dir_path_abs, darshan_file_name),
                              os.path.join(tmp_dir_abs, darshan_darshan)]).wait()

            # Explicitly call the darshan3 logreader and parse the files one by one
            dsh_reader = DarshanLogReader3(tmp_dir_abs, parser=args.darshan_parser)
            parsed_dsh_file = dsh_reader.read_log(darshan_file_full, suggested_label=label)[0].model_job()
            model_jobs.append(parsed_dsh_file)

            # print parsed_dsh_file

    #         fname_list.append(darshan_txt)
    #         dict_name_label[darshan_txt] = label
    #
    # fname_list.sort()
    #
    # # job_map_dataset = profiler_reader.ingest_allinea_profiles(tmp_dir_abs,
    # #                                                           list_json_files=fname_list,
    # #                                                           json_label_map=dict_name_label)
    #
    # dataset = logreader.ingest_data("darshan3", args.path, ingest_config=ingest_config)
    # inested_dsh3_job = Darshan3DataSet.read_log
    #
    # lr = DarshanLogReader3.read_log(tmp_dir_abs, parser=args.darshan_parser)
    # dataset = cls(lr.read_logs(), ingest_path, ingest_config)
    #
    #
    pf = ProfileFormat(model_jobs=model_jobs, workload_tag='allinea_map_files')

    # write output kpf file..
    with open(args.kpf_file, 'w') as f:
        pf.write(f)
