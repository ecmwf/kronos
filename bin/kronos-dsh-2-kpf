#!/usr/bin/env python
# (C) Copyright 1996-2017 ECMWF.
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# In applying this licence, ECMWF does not waive the privileges and immunities
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""
Utility to go through the kronos run folder that has been profiled with Allinea,
and generates a KPF file from the Allinea jsons (requires the map2json tool from Allinea to convert MAP to JSON format)
"""

import os
import subprocess
import json
import argparse
import numpy as np

from kronos.core.logreader.darshan import DarshanLogReader3
from kronos.core.time_signal import time_signal_names, signal_types
from kronos.core.exceptions_iows import ConfigurationError
from kronos.io.profile_format import ProfileFormat


def get_tot_metrics(app_list):
    """
    Return the dictionary with the metrics totals from a json data of a synthetic app..
    :return:
    """

    # make a list out of the input json data..
    app_list = app_list if isinstance(app_list, list) else [app_list]

    metrics_names = time_signal_names
    tot_metrics = {k: np.asarray([]) for k in metrics_names}

    for synth_app in app_list:
        for frame in synth_app["frames"]:

            for ker in frame:

                if ker["name"] == "cpu":
                    tot_metrics["flops"] = np.append(tot_metrics["flops"],
                                                          signal_types["flops"]["type"](ker["flops"]))

                elif ker["name"] == "file-read":
                    tot_metrics["kb_read"] = np.append(tot_metrics["kb_read"],
                                                            signal_types["kb_read"]["type"](ker["kb_read"]))
                    tot_metrics["n_read"] = np.append(tot_metrics["n_read"],
                                                           signal_types["n_read"]["type"](ker["n_read"]))

                elif ker["name"] == "file-write":
                    tot_metrics["kb_write"] = np.append(tot_metrics["kb_write"],
                                                             signal_types["kb_write"]["type"](ker["kb_write"]))
                    tot_metrics["n_write"] = np.append(tot_metrics["n_write"],
                                                            signal_types["n_write"]["type"](ker["n_write"]))

                elif ker["name"] == "mpi":
                    tot_metrics["n_pairwise"] = np.append(tot_metrics["n_pairwise"],
                                                               signal_types["n_pairwise"]["type"](ker["n_pairwise"]))
                    tot_metrics["kb_pairwise"] = np.append(tot_metrics["kb_pairwise"],
                                                                signal_types["kb_pairwise"]["type"](ker["kb_pairwise"]))
                    tot_metrics["n_collective"] = np.append(tot_metrics["n_collective"],
                                                                 signal_types["n_collective"]["type"](
                                                                     ker["n_collective"]))
                    tot_metrics["kb_collective"] = np.append(tot_metrics["kb_collective"],
                                                                  signal_types["kb_collective"]["type"](
                                                                      ker["kb_collective"]))

                # ------------ additional kernels --------
                elif ker["name"] == "fs_metadata":
                    tot_metrics["n_mkdir"] = np.append(tot_metrics["n_mkdir"], int(ker["n_mkdir"]))

                else:
                    raise ("kernel name {} not recognized!".format(ker["name"]))

    return tot_metrics


if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("path_run", type=str, help="Path of the kronos run folder (contains the job-<ID> sub-folders)")
    parser.add_argument("kpf_file", type=str, help="Name of the KPF file to write out")
    parser.add_argument("--darshan_parser", '-d', type=str, help="Path of Darshan parser file")
    args = parser.parse_args()

    if not os.path.exists(args.path_run):
        raise ConfigurationError("Specified run path does not exist: {}".format(args.path_run))

    if not args.darshan_parser:
        raise ConfigurationError("Darshan parser not specified!")

    # path of the output kpf file
    path_kpf = os.path.dirname(os.path.abspath(args.kpf_file))
    if not os.path.exists(path_kpf):
        os.mkdir(path_kpf)

    # create a temporary folder where to store all the converted Darshan files..
    tmp_dir_abs = os.path.join(args.path_run, 'tmp')
    if not os.path.exists(tmp_dir_abs):
        os.mkdir(tmp_dir_abs)

    # ---------- Process the run jsons ----------
    print "writing output kpf file: {}".format(args.kpf_file)
    job_dirs = [x for x in os.listdir(args.path_run) if os.path.isdir(os.path.join(args.path_run, x)) and "job-" in x]

    fname_list = []
    dict_name_label = {}

    model_jobs = []
    for job_dir in job_dirs:

        sub_dir_path_abs = os.path.join(args.path_run, job_dir)
        sub_dir_files = os.listdir(sub_dir_path_abs)
        darshan_file = [f for f in sub_dir_files if f.endswith('.darshan')]

        if darshan_file:

            darshan_file_name = darshan_file[0]
            darshan_file_full = os.path.join(sub_dir_path_abs, darshan_file_name)
            darshan_base_abs = os.path.splitext(darshan_file_full)[0]
            print "processing DARSHAN file: {}".format(darshan_file_full)

            parsed_dsh_file_name = darshan_base_abs + '.txt'
            input_file_path_abs = os.path.join(sub_dir_path_abs, 'input.json')

            # read the corresponding json of the input and read the label
            with open(input_file_path_abs, 'r') as f:
                json_data = json.load(f)

            label = json_data['metadata']['workload_name']
            job_ID = json_data['job_num']

            # copy both the darshan's and the converted txt's into the tmp folder..
            darshan_txt = 'darshan_job-' + str(job_ID) + '.txt'
            darshan_darshan = 'darshan_job-' + str(job_ID) + '.darshan'

            with open(parsed_dsh_file_name, "w") as dsh_f:
                subprocess.Popen([args.darshan_parser,
                                 os.path.join(sub_dir_path_abs, darshan_file_name)],
                                 stdout=dsh_f).wait()

            # Backup files
            subprocess.Popen(['cp',
                              os.path.join(sub_dir_path_abs, parsed_dsh_file_name),
                              os.path.join(tmp_dir_abs, darshan_txt)]).wait()

            subprocess.Popen(['cp',
                              os.path.join(sub_dir_path_abs, darshan_file_name),
                              os.path.join(tmp_dir_abs, darshan_darshan)]).wait()

            # Explicitly call the darshan3 logreader and parse the files one by one
            dsh_reader = DarshanLogReader3(tmp_dir_abs, parser=args.darshan_parser)
            parsed_dsh_model_job = dsh_reader.read_log(darshan_file_full, suggested_label=label)[0].model_job()
            model_jobs.append(parsed_dsh_model_job)

            # Make a quick comparison with what is requested in the input.json
            app_metrics_dict = get_tot_metrics(json_data)
            print "{:<15s};{:>15s};{:>15s}".format("metric", "input.json", "darshan")
            for k in time_signal_names:
                if parsed_dsh_model_job.timesignals[k]:
                    dsh_metric = parsed_dsh_model_job.timesignals[k].sum
                    print "{:15s};{:15.3f};{:15.3f}".format(k, np.sum(app_metrics_dict[k]), dsh_metric)

    # Build the data structure for the KPF file format
    pf = ProfileFormat(model_jobs=model_jobs, workload_tag='allinea_map_files')

    # write output kpf file..
    with open(args.kpf_file, 'w') as f:
        pf.write(f)




