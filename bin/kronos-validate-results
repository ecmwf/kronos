#!/usr/bin/env python
# (C) Copyright 1996-2017 ECMWF.
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# In applying this licence, ECMWF does not waive the privileges and immunities
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""

Script that checks if Kronos has executed a specific metrics as prescribed by the KSchedule file.
It loops over the job output folders, it sums all the values of the selected metric found
in the .KResults files and compares the sum with the sum retrieved from the KSchedule file
(the test is considered PASS if the difference is < 1%)

"""

import os
import json
import argparse
import sys

from kronos.core.time_signal.definitions import signal_types, time_signal_names
from kronos.io.results_format import ResultsFormat
from kronos.io.schedule_format import ScheduleFormat

# map that directly translates the metrics in the KResults
# and the metrics as defined in the KProfile
ts_names_map = {
    "n_write": ("n_write", 1.0),
    "n_read": ("n_read", 1.0),
    "bytes_write": ("kb_write", 1.0/1024.0),
    "bytes_read": ("kb_read", 1.0/1024.0),
    "n_pairwise": ("n_pairwise", 1.0),
    "bytes_pairwise": ("kb_pairwise", 1.0/1024.0),
    "n_collective": ("n_collective", 1.0),
    "bytes_collective": ("kb_collective", 1.0/1024.0),
    "flops": ("flops", 1.0),
}


def cumsum(input_list):
    return [sum(input_list[:ii+1]) for ii,i in enumerate(input_list)]


def kresults_to_profiled_job_json(kresults_json, input_json):
    """
    Function that reads statistics.kresults and input.json and builds job json data (to be later used for ProfileFormat)
    :param kresults_json:
    :param input_json:
    :return:
    """

    job = {}

    # create time series json data from statistics.kresults
    time_series_json = {}

    for rank_data in kresults_json["ranks"]:

        tends = cumsum(rank_data["time_series"]['durations'])

        if not rank_data["time_series"]['durations']:
            # in case the durations are empty!
            return None

        for ts_name, ts_vals in rank_data["time_series"].iteritems():

            if ts_name != "durations":
                time_series_json.setdefault(ts_name, []).extend(zip(tends, ts_vals))

    # sort the time-series
    for ts in time_series_json.values():
        ts.sort(key=lambda _x: _x[0])

    # time delay of job submission
    if input_json.get("start_delay"):
        job['time_start'] = input_json.get("start_delay")

    # job runtime
    job['duration'] = max([v[0] for ts in time_series_json.values() for v in ts])

    if input_json.get("start_delay"):
        job['ncpus'] = input_json.get("num_procs")

    if input_json.get("metadata"):
        job['label'] = input_json["metadata"]["workload_name"]

    if input_json.get("metadata"):
        job['job_name'] = input_json["metadata"]["job_name"]

    # Append any time series data that is present
    time_series = {}
    for name, values in time_series_json.iteritems():

        # assert name in time_signal.signal_types
        assert name in ts_names_map.keys()

        if values is not None:
            time_series[ts_names_map[name][0]] = {
                'times': list(zip(*values)[0]),
                'values': [v*ts_names_map[name][1] for v in list(zip(*values)[1])],
            }

    # The time series data is only included if it is present.
    if time_series:
        job['time_series'] = time_series

    return job


if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("job_dir", type=str, help="Path of the kronos output folder 'job_dir' (contains the job-<ID> sub-folders)")
    parser.add_argument("metric_name", type=str)

    # print the help if no arguments are passed
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    # parse the arguments..
    args = parser.parse_args()

    # check if metric name exists..
    if args.metric_name not in [k[0] for k in ts_names_map.values()]:
        print "metric name not existent!"

    # check that the run path exists
    if not os.path.exists(args.job_dir):
        print "Specified run path does not exist: {}".format(args.job_dir)
        sys.exit(1)

    # check that the run path contains the job sub-folders
    job_dirs = [x for x in os.listdir(args.job_dir) if os.path.isdir(os.path.join(args.job_dir, x)) and "job-" in x]
    if not job_dirs:
        print "Specified path does not contain any job folder (<job-ID>..)!"
        sys.exit(1)

    # check that the run path contains the kschedule file
    kschedule_file = [x for x in os.listdir(args.job_dir) if os.path.isfile(os.path.join(args.job_dir, x)) and x.endswith('.kschedule')]
    if not kschedule_file:
        print "Specified path does not contain the KSchedule file!"
        sys.exit(1)

    if len(kschedule_file) > 1:
        print "Specified path contains more than one KSchedule file!"
        sys.exit(1)

    # sums of all metrics for all the jobs
    kresults_metrics_sums = {k[0]: 0.0 for k in ts_names_map.values()}

    # loop over all sub-folders
    for job_subdir in job_dirs:

        # read the statistics.kresults file
        kresults_file_path_abs = os.path.join(os.path.join(args.job_dir, job_subdir), "statistics.kresults")
        if not os.path.isfile(kresults_file_path_abs):
            sys.exit(1)

        kresults_data_json = ResultsFormat.from_filename(kresults_file_path_abs).output_dict()

        # read the input.json file
        input_file_path_abs = os.path.join(os.path.join(args.job_dir, job_subdir), 'input.json')

        # read the corresponding json of the input and read the label
        with open(input_file_path_abs, 'r') as f:
            input_data_json = json.load(f)

        job_json = kresults_to_profiled_job_json(kresults_data_json, input_data_json)

        # print json.dumps(job_json, sort_keys=True, indent=4, separators=(',', ': '))
        for metric,time_values in job_json["time_series"].iteritems():
            kresults_metrics_sums[metric] += sum(time_values["values"])

    # read data from the KSchedule
    kschedule_data = ScheduleFormat.from_filename(os.path.join(args.job_dir, kschedule_file[0]))
    tot_kschedule_metrics = {k: signal_types[k]["type"](0) for k in time_signal_names}

    for synth_app in kschedule_data.synapp_data:
        for frame in synth_app["frames"]:

            for ker in frame:

                if ker["name"] == "cpu":
                    tot_kschedule_metrics["flops"] += signal_types["flops"]["type"](ker["flops"])

                elif ker["name"] == "file-read":
                    tot_kschedule_metrics["kb_read"] += signal_types["kb_read"]["type"](ker["kb_read"])
                    tot_kschedule_metrics["n_read"] += signal_types["n_read"]["type"](ker["n_read"])

                elif ker["name"] == "file-write":
                    tot_kschedule_metrics["kb_write"] += signal_types["kb_write"]["type"](ker["kb_write"])
                    tot_kschedule_metrics["n_write"] += signal_types["n_write"]["type"](ker["n_write"])

                elif ker["name"] == "mpi":
                    tot_kschedule_metrics["n_pairwise"] += signal_types["n_pairwise"]["type"](ker["n_pairwise"])
                    tot_kschedule_metrics["kb_pairwise"] += signal_types["kb_pairwise"]["type"](ker["kb_pairwise"])
                    tot_kschedule_metrics["n_collective"] += signal_types["n_collective"]["type"](ker["n_collective"])
                    tot_kschedule_metrics["kb_collective"] += signal_types["kb_collective"]["type"](ker["kb_collective"])

                # ------------ additional kernels --------
                elif ker["name"] == "fs_metadata":
                    tot_kschedule_metrics["n_mkdir"] += int(ker["n_mkdir"])
                else:
                    raise ("kernel name {} not recognized!".format(ker["name"]))

    # if the metrics is zero (both in the KResults's and in the KSchedule) => check is PASS
    if abs(kresults_metrics_sums[args.metric_name] == 0 and tot_kschedule_metrics[args.metric_name]) == 0:
        print "Result for metric {}: PASS".format(args.metric_name)
        sys.exit(0)

    # fails if the requested metrics differ from the measured one
    if abs(kresults_metrics_sums[args.metric_name] - tot_kschedule_metrics[args.metric_name])/tot_kschedule_metrics[args.metric_name] > 0.01:
        sys.exit(1)
    else:
        print "Result for metric {}: PASS".format(args.metric_name)
        sys.exit(0)




