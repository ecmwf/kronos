# (C) Copyright 1996-2017 ECMWF.
# 
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0. 
# In applying this licence, ECMWF does not waive the privileges and immunities 
# granted to it by virtue of its status as an intergovernmental organisation nor
# does it submit to any jurisdiction.

"""

============================================================================
       **DEVELOPMENT TOOL** - USAGE OF THIS TOOL IS UNSUPPORTED
============================================================================

This tool generates an array of workflows from an initial schedule (and dependencies)
the resulting workflows will be either concatenated as a series of dependent workflows or
executed in parallel

Input Parameters:

    - input_kschedule_path: Kronos input kschedule (from which job information are taken)

    - quantity: Node weight quantity (a quantity that is used to "weight" each dependency depth level)

    - overlap: Workflows concatenation overlap, in [0,1] (overlap between serially concatenated workflows)

    - kill: Workflows kill fraction, in [0,1] (fraction of jobs to be removed from the workload tree)

    - serial: Number of serially dependent workflows

    - parallel: Number of parallel workflows

    - output: Name of output kschedule

"""

import os
import sys
import copy
import json
import argparse
import networkx as nx


def build_graph_from_workflow_prototype(kschedule_json_data, qty_id):
    """
    Build graph from single kschedule prototype
    :param kschedule_json_data:
    :param qty_id:
    :return:
      - graph (the actual graph)
      - longest_path_to_root (list of longest path to a root node for each node of the graph)
    """

    # TODO: consider also other quantities to weigh jobs
    weight_qty_map = {
        "N": "num_procs"
    }

    print "Creating prototype workflow from input kschedule.."

    # Graph of prototype workflow
    G = nx.DiGraph()

    # take the job sof this specific workload
    jobs = kschedule_json_data['jobs']

    # Fill in the dependencies for these jobs
    for jj, job in enumerate(jobs):

        deps = job['depends']
        jobname = job["metadata"]["workload_name"]

        # Add node (and parent nodes) into the graph
        G.add_node(jj, jobname=jobname, start_delay=job['start_delay'], n_procs=job[weight_qty_map[qty_id]], orig_idx=jj)

        # For each of the depend nodes add node + directed edge (depend -> node)
        for d in deps:
            d_name = jobs[d]["metadata"]["workload_name"]
            t_del = jobs[d]['start_delay']
            n_proc = jobs[d]['num_procs']
            G.add_node(d, jobname=d_name, start_delay=t_del, n_procs=n_proc, orig_idx=d)
            G.add_edge(d, jj)

    # calc tree roots
    root_nodes = []
    for n in G:
        if not nx.ancestors(G, n):
            root_nodes.append(n)

    # dictionary of node ancestors
    ancestors_dict = {}
    for n in G:
        ancestors_dict[n] = nx.ancestors(G, n)

    # # ===========================================
    # # subdivision of nodes by dependency-depth level
    # longest_path_to_root = {}
    # for n in G:
    #     if n not in root_nodes:
    #
    #         print "Calculating depth level for node: {}".format(n)
    #         node_ancestors = ancestors_dict[n]
    #         path_lenghts = {}
    #         for i_root in root_nodes:
    #             if i_root in node_ancestors:
    #                 path_lenghts[i_root] = nx.shortest_path_length(G, source=i_root, target=n)
    #
    #         best_canditdate_for_farthest_root = max(path_lenghts, key=path_lenghts.get)
    #
    #         all_simple_path = nx.all_simple_paths(G, source=best_canditdate_for_farthest_root, target=n)
    #
    #         # longest path to any root node
    #         longest_path_to_root[n] = max([len(p) for p in all_simple_path])
    #     else: # root nodes have no parents => 0 distance
    #         longest_path_to_root[n] = 0

    # # ============= for testing only ============
    # import pickle
    # with open('/home/ma/maab/workspace/itt_1_tools/lp_to_root.pickle', 'wb') as f:
    #     pickle.dump(longest_path_to_root, f)
    # ---------------------
    import pickle
    longest_path_to_root = pickle.load(open('/home/ma/maab/workspace/itt_1_tools/lp_to_root.pickle'))
    # ===========================================

    # number of "processors" summed up per graph depth level
    node_depth_sums = [0] * max(longest_path_to_root.values())
    for n in G:
        node_lvl_idx = longest_path_to_root[n]-1 if longest_path_to_root[n] else 0
        node_depth_sums[node_lvl_idx] += G.node[n]["n_procs"]

    print "Grouped #processors per depth level"
    print "\n".join(["lvl-{}: {}".format(ssi, ss) for ssi, ss in enumerate(node_depth_sums)])

    return G, longest_path_to_root


def concatenate_workflows(G_proto, nd_lvl, overlap, kill_head, kill_tail, n_serial):
    """
    Concatenate workflows in series, including overlapping
    :param G_proto:
    :param nd_lvl:
    :param overlap:
    :param n_serial:
    :return:
    """

    # 1) Logic to decide the actual kill and overlap depth level indexes
    # (we need to take into account that we cannot kill a portion of workflow jobs
    # bigger than the portion we are going to overlap..)

    # idx of node to which the subsequent graphs will be attached to
    link_lvl_idx = int(max(nd_lvl.values()) * (1 - overlap))
    link_lvl_idx = min(link_lvl_idx, len(nd_lvl) - 1)
    link_lvl_idx = max(link_lvl_idx, 0)

    # Initial attempted value of kill_lvl_idx
    kill_lvl_idx = int(max(nd_lvl.values()) * kill_head)

    # boundary cases
    if link_lvl_idx == 0:  # 1) left case (overlap=1 => linking_level=0)

        # cannot kill any portion of
        # the workflow (we are linking at level 0)
        kill_lvl_idx = 0

    else:

        # kill level can be at most at one level before link_lvl_idx
        kill_lvl_idx = min(kill_lvl_idx-1, link_lvl_idx-1)
        kill_lvl_idx = max(kill_lvl_idx, 0)

    print "link_lvl_idx: {}".format(link_lvl_idx)
    print "kill_lvl_idx: {}".format(kill_lvl_idx)

    # 2) now remove all the nodes from the first workflow whose level is less than kill_lvl_idx
    truncated_G_first = truncate_workflow_head(G_proto, kill_lvl_idx, nd_lvl)

    # 3) Finally start concatenating a series of workflows (copy of the prototype workflow)
    # but attached to the truncated version of the prototype workflow
    concatenated_G = copy.deepcopy(truncated_G_first)
    print "1) attaching {} nodes from workflow {}".format(len(concatenated_G), "truncated_G_first")

    # Choose the first point whose level idx is closest to the overlap level index
    tot_num_nodes_g_proto = len(G_proto)
    ovl_node_source = None
    for n in G_proto:
        if nd_lvl[n] == link_lvl_idx:
            ovl_node_source = n
            break

    # print "linking node: {}".format(ovl_node_source)

    # Make sure we found a candidate as linking node
    assert ovl_node_source

    # Also, make sure that the linking node is still in the truncated first workflow
    assert truncated_G_first.has_node(ovl_node_source)

    # 1) *** (n_serial-2) because we are already starting from the proto workflow..) and the last workflow ***
    # needs to be truncated by "kill_tail" before being attached
    for i_serial in range(n_serial-2):

        print "{}): attaching {} nodes".format(i_serial+2, len(G_proto))

        # Create another graph with the same structure of the initial one but relabelled nodes
        i_serial_graph = nx.relabel_nodes(G_proto, {n: n + tot_num_nodes_g_proto * (i_serial + 1) for n in G_proto}, copy=True)

        # Attach the newly created graph to the initial one
        concatenated_G = nx.union(concatenated_G, i_serial_graph)

        # Calc tree roots (these will be the sinks for links with previous graph..)
        root_nodes = []
        for n in i_serial_graph:
            if not nx.ancestors(i_serial_graph, n):
                root_nodes.append(n)

        # Attach root nodes to previous graph linking node
        for r in root_nodes:
            concatenated_G.add_edge(ovl_node_source, r)

        # update source node
        ovl_node_source += tot_num_nodes_g_proto

    # 2) *** Now truncate and attach the LAST workload of the series.. ***
    i_serial = n_serial-2

    # Create another graph with the same structure of the initial one but relabelled nodes
    i_serial_graph = nx.relabel_nodes(G_proto, {n: n + tot_num_nodes_g_proto * (i_serial + 1) for n in G_proto}, copy=True)

    # idx of node from which the tail will be cut off
    kill_tail_lvl_idx = int(max(nd_lvl.values()) * kill_tail)

    kill_tail_lvl_idx = min(kill_tail_lvl_idx, len(nd_lvl) - 1)
    kill_tail_lvl_idx = max(kill_tail_lvl_idx, 0)
    first_node_offset = G_proto.nodes()[0] + tot_num_nodes_g_proto * (i_serial + 1)

    # pass the kill level to the truncation function for chopping off a section of the last workflow..
    last_graph_truncated = truncate_workflow_tail(i_serial_graph, kill_tail_lvl_idx, nd_lvl, first_node_offset)

    print "{}): attaching {} nodes from workflow {}".format(n_serial, len(last_graph_truncated), "last_graph_truncated")

    # Attach the newly created graph to the initial one
    concatenated_G = nx.union(concatenated_G, last_graph_truncated)

    # Calc tree roots (these will be the sinks for links with previous graph..)
    root_nodes = []
    for n in last_graph_truncated:
        if not nx.ancestors(last_graph_truncated, n):
            root_nodes.append(n)

    # Attach root nodes to previous graph linking node
    for r in root_nodes:
        concatenated_G.add_edge(ovl_node_source, r)

    # update source node
    ovl_node_source += tot_num_nodes_g_proto

    return concatenated_G


def multiply_workflows(_G, n_parallel):
    """
    Add parallel repetitions of workflows
    :param G:
    :param n_parallel:
    :return:
    """

    concatenated_G = copy.deepcopy(_G)
    tot_num_nodes = len(concatenated_G)

    node_id_offset = max([n for n in concatenated_G.node])

    # (n_parallel-1 because we are already starting from one series of workflow..)
    for i_parallel in range(n_parallel-1):

        i_parallel_graph = nx.relabel_nodes(_G, {n: n+node_id_offset*(i_parallel+1) for n in _G}, copy=True)

        # Attach the newly created graph to the initial one
        concatenated_G = nx.union(concatenated_G, i_parallel_graph)
        # concatenated_G = nx.disjoint_union(concatenated_G, i_parallel_graph)

    return concatenated_G


def truncate_workflow_head(_G, head_kill_idx, nd_lvl):
    """
    Truncate a workflow head at a specific dependency level
    :param _G:
    :param head_kill_idx:
    :param nd_lvl:
    :return:
    """

    print "Truncating head.."
    assert 0 <= head_kill_idx <= len(nd_lvl) - 1

    truncated_G = copy.deepcopy(_G)

    _removed = []
    for n in _G:
        if nd_lvl[n] < head_kill_idx:
            truncated_G.remove_node(n)
            _removed.append(n)

    print "--> Removed {} head nodes".format(len(_removed))
    return truncated_G


def truncate_workflow_tail(_G, tail_kill_idx, nd_lvl, _offset):
    """
    Truncate a workflow tail at a specific dependency level
    :param _G:
    :param tail_kill_idx:
    :param nd_lvl:
    :return:
    """

    print "Truncating tail.."
    assert 0 <= tail_kill_idx <= len(nd_lvl) - 1

    truncated_G = copy.deepcopy(_G)

    _removed = []
    for n in _G:
        if nd_lvl[n-_offset] > tail_kill_idx:
            truncated_G.remove_node(n)
            _removed.append(n)

    print "--> Removed {} tail nodes".format(len(_removed))
    return truncated_G


def generate_json_jobs_from_graph(_G, proto_json_data):
    """
    Reganerate kschedule jobs from graph
    :param _G:
    :param proto_json_data:

    :return:
    """

    _proto_jobs_lookup = {j["metadata"]["workload_name"]: j for j in proto_json_data["jobs"]}
    _jobs_all = []

    nodes_in_G = [n for n in _G.node]

    for nn, n in enumerate(_G):
        job_name = _G.node[n]["jobname"]
        job_template = copy.deepcopy(_proto_jobs_lookup[job_name])

        # dependencies
        # job_template["depends"] = _G.predecessors(n)
        pred_n = _G.predecessors(n)
        dep_position_in_list = [nodes_in_G.index(nd) for nd in pred_n]
        job_template["depends"] = dep_position_in_list

        job_template["metadata"]["job_name"] = "appID-{}".format(nn)
        _jobs_all.append(job_template)

    return _jobs_all


if __name__ == "__main__":

    # Parser for the required arguments
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("input_kschedule_path", type=str, help="Kronos input kschedule")

    parser.add_argument("-q", "--quantity", type=str, help="Node weight quantity", default="N")

    parser.add_argument("-l", "--overlap", type=float, help="Workflows concatenation overlap, in [0,1]", default=0.5)

    parser.add_argument("-kh", "--kill_head", type=float, help="Workflows head kill fraction, in [0,1]: "
                                                               "removed fraction of first concatenated workload", default=0.5)

    parser.add_argument("-kt", "--kill_tail", type=float, help="Workflows tail kill fraction, in [0,1]: "
                                                               "removed fraction of last concatenated workload", default=0.5)

    parser.add_argument("-s", "--serial", type=int, help="Number of serially dependent workflows", default=1)

    parser.add_argument("-p", "--parallel", type=int, help="Number of parallel workflows", default=3)

    parser.add_argument("-o", "--output", type=str, help="Name of output kschedule", default="output.kschedule")

    # print the help if no arguments are passed
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    # parse the arguments..
    args = parser.parse_args()

    # some checks
    if not os.path.exists(args.input_kschedule_path):
        print "Specified path does not exist: {}".format(args.input_kschedule_path)
        sys.exit(1)

    # load the file
    with open(args.input_kschedule_path, 'r') as f:
        kschedule_json_data_orig = json.load(f)

    # check that the chopping options are sensible..
    assert 0 <= args.kill_head <= 1
    assert 0 <= args.kill_tail <= 1

    # Build the first prototypical workflow
    proto_workflow_graph, node_depth_lvl = build_graph_from_workflow_prototype(kschedule_json_data_orig, args.quantity)
    print "len proto workflow.. {}".format(len(proto_workflow_graph))

    # concatenate serial repetitions of workflows (one after another..)
    serial_workflows = concatenate_workflows(proto_workflow_graph,
                                             node_depth_lvl,
                                             args.overlap,
                                             args.kill_head,
                                             args.kill_tail,
                                             args.serial)

    print "len serial workflow.. {}".format(len(serial_workflows))

    # multiply workflows in parallel
    # all_workflows = serial_workflows
    all_workflows = multiply_workflows(serial_workflows, args.parallel)
    print "len serial+parallel workflow.. {}".format(len(all_workflows))

    # check final graph for self-loops
    print "Total N graph cycles: {}".format(len(list(nx.simple_cycles(all_workflows))))

    # write all the jobs to an output schedule
    ksf_template = {k: v for k, v in kschedule_json_data_orig.iteritems() if k != 'jobs'}
    ksf_template["jobs"] = generate_json_jobs_from_graph(all_workflows, kschedule_json_data_orig)

    # write all the jobs to the output kschedule..
    with open(args.output, 'w') as f:
        json.dump(ksf_template, f, sort_keys=True, indent=4, separators=(',', ': '))
        # json.dump(ksf_template, f, sort_keys=True, separators=(',', ': '))
